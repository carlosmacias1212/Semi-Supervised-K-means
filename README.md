# Semi-Supervised-K-means
Modify a K-means algorithm implementation to support "must-link" constraints.

In the homework, we will in particular explore a semi-supervised extension of the k-means algorithm. The k- means algorithm produces a clustering from a given dataset, and is considered an unsupervised learning algorithm. In a semi-supervised extension, we shall provide “hints” to the k-means algorithm, to help it produce an improved clustering. A data scientist may want to provide some partial clustering information to help the k-means algorithm find better clusters. Such partial information could be in the form of a “must-link” constraint, that asserts that a pair of images should be assigned to the same cluster. It may also come in the form of a “must-notlink” constraint, that asserts that two examples should be assigned to two different clusters. When we take advantage of this type of partial information about clusters, we say that we are performing “semi-supervised” clustering, in contrast to the usual “unsupervised” clustering when we do not have such partial information.

## Consider the implementation of the k sets() function, where we pick the initial cluster centers, from the list of constraints. When we have no “must-link” constraints, then it is typical to just select k images from the dataset at random, and use the locations as initial cluster centers. However, when we use “must-link” constraints, the “must-link” constraints provides some partial information about the clusters, which we could take advantage of when selecting initial cluster centers. Consider what happens as we increase the number of “must-link” constraints that we provide. With few constraints, most images are independent, and the ones that are linked together are linked to a small number of other images. With many constraints, few images are independent, and the ones that are linked together are linked to a large number of other images. Based on this observation, provide an argument that we should pick the k-largest sets of linked images to serve as initial cluster centers.
It would make sense to use the k-largest sets of linked images as the initial cluster centers, because that would maximize the number of images that can be quickly grouped into relevant clusters. If most of the images are linked to other images, then picking the largest groups of linked images and making cluster centers out of them will help minimize the amount of clustering that will need to be done with the remaining images. It will also increase the chances of yielding relatively even clusters.

## Consider the implementation of the summarize set(constraint) function. In the e-step of the kmeans algorithm, given a set of images that are constrained to be in the same cluster, we need to find a single cluster center to assign the entire set of images to. Normally, we would simply assign an image to its closest cluster. However, every image that is linked together by a “must-link” constraint should be assigned to the same cluster. To simplify this task, given a set of images that are linked together by a “must-link” constraint, we can somehow pick a single image to represent the entire set. We find the cluster that the single representative image is closest to, and every image linked together is assigned to that same closest cluster. Propose a way to pick such a representative image, and provide an argument for why it might work well.
One way to pick a representative image of a set of “must-link” images is to take the mean of the set using the numpy.mean function. This will return an image that is roughly representative of all the
images that are passed to it and in turn will provide a means to classify the entire set to a cluster that most closely matches the average image found within that constraint of images.

## Once you have implemented the k sets() and summarize set(constraint) functions, as discussed in the previous two questions, try running k-means clustering with 100 random “must-link” constraints. Include a picture of the 10 found clusters. Do similarly with 1,000 constraints, 10,000 constraints and 100,000 constraints (include a picture of the resulting clusters in each case). Do the 10 clusters found tend to become the 10 expected digits as you increase the number of “must-link” constraints? You may want to try multiple random number seeds, to see the effect of different runs.
100 Constraints
<img width="500" alt="Screenshot 2023-11-18 at 6 32 14 PM" src="https://github.com/carlosmacias1212/Semi-Supervised-K-means/assets/68452096/b6a76c45-618b-401b-ab42-8a8961a78ce9">

1000 Constraints
<img width="500" alt="Screenshot 2023-11-18 at 6 34 42 PM" src="https://github.com/carlosmacias1212/Semi-Supervised-K-means/assets/68452096/9e744952-b754-45e6-861b-d6055d50158c">

10000 Constrains
<img width="600" alt="Screenshot 2023-11-18 at 6 36 07 PM" src="https://github.com/carlosmacias1212/Semi-Supervised-K-means/assets/68452096/e5462c42-7244-4228-889c-bc0af1b12c39">

100000 Constraints
<img width="500" alt="Screenshot 2023-11-18 at 6 36 54 PM" src="https://github.com/carlosmacias1212/Semi-Supervised-K-means/assets/68452096/224f07d2-00df-4d7e-b45f-4e0e55308b8c">
